# LLM Paper Reproduction and Implementation
This repository is dedicated to reproducing and implementing various research papers, neural network architectures, and techniques related to Large Language Models (LLMs). The goal is to help deepen understanding of LLM concepts, and assist with learning by implementing and experimenting with state-of-the-art models and methods.

## Contents
Papers: Reproduced or implemented models from recent research papers.
Neural Networks: Various architectures, layers, and model implementations.
Techniques: Code and experiments related to techniques like attention mechanisms, transformers, fine-tuning, etc.
Datasets: A collection of datasets used for training and testing the models.
Experiments: Notes and results from experiments conducted during the learning process.


# Resources
1. [Andrej Karpanthy Bigram Language Model](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=hoelkOrFY8bN)
2. [NanoGPT](https://github.com/karpathy/nanoGPT)
3. LoRA
